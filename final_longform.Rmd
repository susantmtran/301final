---
title: "Final Project for STAT 301-3"
subtitle: "Spring 2021"
author: "Grace Park, Susan Tran, Nomi Tannenbaum, Jamie Lee, Alex Chang"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

# *Introduction*

```{r}
# load packages
library(tidyverse)

# set seed
set.seed(101)

# load data
df <- read_csv("data/unprocessed/BigML_Dataset_5f50a62c2fb31c516d000176.csv")
```

We discovered our data set through the BigML platform, which is considered to be a powerful machine learning service in the data science world. There are [multiple data sets](https://bigml.com/user/academy_awards/gallery/datasets) related to the Academy Awards on BigML, whose users scraped the data from [IMDb](https://www.imdb.com/), a well-known online database on movies, television shows, and so forth. We are focusing the [2000-2017](https://bigml.com/user/academy_awards/gallery/dataset/5a94302592fb565ed400103b) time period of nominations and awards, and plan to predict whether or not a film has received an Oscar nomination (in any category) depending on a few select variables. We chose this data set due to our shared interest in film as well as our curiosity regarding how well a nomination could be predicted when limited to the variables in the data.

# *Exploratory Data Analysis*

To start off our exploratory data analysis, we will look at the distribution of the outcome variable.

```{r}
df$Oscar_nominated <- ifelse(df$Oscar_nominated == 0, 0, 
                                         ifelse(df$Oscar_nominated != 0, 1, NA))

df %>% 
  ggplot(aes(as_factor(Oscar_nominated))) +
  geom_bar(fill = c("#008f69", "#2f84de"), alpha = 0.5) +
  scale_x_discrete("Oscar Nominated", labels = c("0" = "No", "1" = "Yes")) +
  labs( title = "Distribution of Outcome Variable: Oscar Nominated",
        y = "Count") +
  theme_classic()
```

We can see that the distribution of our outcome variable is slightly skewed with more movies having Oscar nominations. We will take this into account when splitting our data and will be sure to stratify based on our outcome variable.

Next we will observe the missing values, if any, using the `skimr` and `naniar` packages. The essential component of feature engineering for models is *non*-missingness, so it important that we address missing values as soon as possible.

## Missingness

```{r results='hide'}
skimr::skim_without_charts(df)
```

```{r}
naniar::miss_var_summary(df)
```

Given the difficulty that we had with finding a data set that would work well for project and because there are a little over 100 variables for about 1000 movies, it is unsurprising that there is a significant amount of missing values in the data set. However, after taking a closer look, the variables with the highest levels of missingness are those indicating which categories movies are nominated for, and which categories movies have won awards in. Understandably, if a movie is not nominated and therefore not a winner, these categories are nonexistent and therefore irrelevant. As result, these variables will be unhelpful in creating our models and should not be included in our recipes.

Without those variables in mind, the highest percentage of missingness is 10% for the `popularity` variable. The general rule of thumb for missingness is about 20%, so we are good to consider the remaining variables and likely impute during the recipe steps to solve any additional problems.

Note that the results of `skimr::skim_without_charts()` are hidden due to the length and irrelevance to the overall long form. Now that we are aware of the missing values, we can take a closer look at the different variables in the data set and gauge their potential usefulness in our recipe(s).

## Trends

```{r}
df %>% ggplot(aes(awards_nominations, awards_wins)) +
  geom_jitter(aes(color = Oscar_Best_Picture_nominated), alpha = 0.5) +
  scale_color_manual("Nominated for Oscars Best Picture?",
                     values = c("#008f69", "#2f84de"),
                     labels = c("NO", "YES")) +
  geom_smooth(color = "black") +
  labs(title = "Cumulative Nominations and Awards",
       subtitle = "Films Released during 2000–2017",
       x = "Total Nominations",
       y = "Total Wins",
       caption = "For The Academy Awards, Golden Globes, BAFTA, and other prestigious
       film critic organizations.") +
  theme_classic() +
  theme(legend.position = "top",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```

In the plot above, we can observe that the total number of nominations that a film receives is positively correlated with its number of wins. We can also observe that films nominated for The Oscars Best Picture award tend to be on the right side of the plot. Understandably, films that are well-crafted enough to receive a Best Picture nomination will most likely have considerable nominations and wins in different categories and award shows.

## Notable Elements

```{r}
df %>% ggplot(aes(rate, metascore)) +
  geom_jitter(alpha = 0.5, color = "#008f69") +
  geom_smooth(color = "black") +
  labs(title = "IMDb Rating vs Metascore",
       subtitle = "Films Released during 2000–2017",
       x = "IMDb Rating",
       y = "Metascore",
       caption = "The metascore is a weighted average of many reviews from reputable
       critics.") +
  theme_classic() +
  theme(legend.position = "top",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```

IMDb ratings reflect their user input, which does not necessarily come from renowned and/or prestigious film critics. In contrast, the metascore is a conglomerate of the different famous film critic organizations. Again, these two values are positively correlated but with enough variation so that they are distinct enough to use in our recipe(s). `step_corr()` might even be useful.

```{r}
df %>% count(release_date.month, Oscar_Best_Picture_nominated) %>% drop_na() %>% 
  filter(Oscar_Best_Picture_nominated == "Yes") %>% 
  select(1, 3) %>% 
  ggplot(aes(release_date.month, n)) +
  geom_col(aes(fill = release_date.month)) +
  scale_x_continuous(breaks = seq(1, 12, 1)) +
  labs(title = "Total Number of Oscar Best Picture Nominees, By Month of Release",
       subtitle = "Films Released during 2000–2017",
       x = "Calendar Month",
       y = NULL) +
  theme_classic() +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

df %>% count(release_date.month, Oscar_Best_Picture_won) %>% drop_na() %>% 
  filter(Oscar_Best_Picture_won == "Yes") %>% 
  select(1, 3) %>% 
  ggplot(aes(release_date.month, n)) +
  geom_col(aes(fill = release_date.month)) +
  scale_x_continuous(breaks = seq(1, 12, 1)) +
  labs(title = "Total Number of Oscar Best Picture Winners, By Month of Release",
       subtitle = "Films Released during 2000–2017",
       x = "Calendar Month",
       y = NULL) +
  theme_classic() +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```

Interestingly, the distribution for the total number of Oscar Best Picture nominees/winners over the course of the calendar year is bimodal, with some significant points of "missing data". For reference, during 2000--2017, the period of eligibility for each award show is the full previous calendar year from January 1st to December 31st. The 2017 Academy Awards considered films released between January 1, 2016 to December 31, 2016, for example. Nominations are often announced in January (right after the cutoff date) and the show occurs at the end of February.

Notably, a majority of the nominees and winners were released in October, November, and December. This is mostly due to having a release so close to the cutoff date, which ups the memorability and relevance of the film by the time the nominations come in. There are also a significant number of nominees and winners released in the month of January. While that is a considerably long time from the end of eligibility period, the culture surrounding media during the new year probably increases a film's longevity in the public opinion.

The month of release could be an important factor in our predictions. However, it tends to be difficult to smoothly incorporate date variables into recipes (even with `step_date()`), so we should keep that in mind before committing too heavily to this variable.

## *Next Steps*

Now that we have performed a thorough exploratory analysis on our data set, we are more understanding of the different variables and therefore better equipped to feature engineer and develop a well-performing model. Since this is a classification problem, we will focus on the boosted tree, multilayer perceptron, neural network, logistic regression, and random forest algorithms, which have all performed well on our labs in the past.

# *Machine Learning*

In order to produce the best results, we tested five different models: K-nearest neighbor, boosted tree, random forest, elastic net, and neural net. Using cross validation, we tested each model with two different recipes, tuning different model parameters on the folds. We found that the second recipe was more successful for every model and using this recipe we obtained the following results:

```{r}
library(tidyverse)
library(tidymodels)


rf_tuned <- read_rds("model-info/tuned/rf_tuned_best.rds")
nn_tuned <- read_rds("model-info/tuned/nn_tuned_best.rds")
bt_tuned <- read_rds("model-info/tuned/bt_tuned_best.rds")
mlp_tuned <- read_rds("model-info/tuned/mlp_tuned_best.rds")
en_tuned <- read_rds("model-info/tuned/en_tuned_best.rds")

tune_results <- tibble(
  model_type = c("rf_tuned", "nn_tuned", "mlp_tuned", "en_tuned", "bt_tuned"),
  tune_info = list(rf_tuned, nn_tuned, mlp_tuned, en_tuned, bt_tuned),
  assessment_info = map(tune_info, collect_metrics),
  best_model = map(tune_info, ~select_best(.x, metric = "accuracy")))

tune_results %>%
  select(model_type, assessment_info) %>%
  unnest(assessment_info) %>%
  filter(.metric == "accuracy") %>%
  group_by(model_type) %>%
  summarise(accuracy = max(mean)) %>% 
  arrange(desc(accuracy))
```

Based on these findings, random forest was our best performing model. Next we fit this winning model on the entire training set and then the testing set.

## *Feature Engineering, Data Splitting, and Model Fitting*

load all of our thingies in data/processed

## *Results*

Once we fit our winning random forest model on the training set and then the testing set, we obtained the following results:

```{r}
rf_training_fit <- read_rds("model-info/rf_best_model_training_fit.rds")
oscar_test <- readRDS("data/processed/oscar_test.rds")

rf_accuracy <- rf_training_fit %>% 
  predict(oscar_test) %>% 
  bind_cols(truth = oscar_test$Oscar_nominated) %>% 
  accuracy(truth = truth, estimate = .pred_class) 

rf_training_fit %>% 
  predict(new_data = oscar_test, type = "prob") %>% 
  bind_cols(truth = oscar_test$Oscar_nominated) %>% 
  roc_auc(truth = truth, .pred_0) %>% # roc-auc
  bind_rows(rf_accuracy) %>% # elastic net accuracy (stored)
  mutate(
    Metric = .metric, # rename
    Estimate = .estimate
  ) %>% 
  select(Metric, Estimate) # select

rf_training_fit %>% 
  predict(new_data = oscar_test, type = "prob") %>% 
  bind_cols(truth = oscar_test$Oscar_nominated) %>% 
  roc_curve(truth = truth, .pred_0) %>% 
  autoplot()
```

We included both ROC-AUC and accuracy as performance metrics, seeing a slight decrease in performance for the accuracy, although it is still relatively high. The area under the ROC-AUC curve is also fairly high at 0.923 out of 1, which is a numerical measure of how good the model is at classifying between whether or not the film received an Oscar nomination.

# *Conclusion*

Overall, we are pleased with our model's ability to predict a film's Oscar nomination or lack thereof, especially given that there are various factors that lead to a nomination that may not be measurable with predictors in a data set. A codebook is an additional resource that could have helped improve model performance - our data source did not provide one and we therefore did not gain as much context into the variables we were working with as we had initially hoped, leaving us to our speculation. Although most were relatively straightforward which allowed us to make reasonable assumptions, next steps could include gaining that additional information which may be useful.

Add more?
